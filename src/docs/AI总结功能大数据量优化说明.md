# AI 总结功能大数据量优化说明

## 问题背景

在原始实现中，AI 总结功能存在以下潜在问题：

1. **数据量过大**：当项目数量很多或数据量很大时，一次性将所有数据传给 AI API，可能导致：
   - Prompt 超过模型的 token 限制（如 GPT-3.5-turbo 约 4096 tokens）
   - 处理时间过长，超过 API 超时限制
   - API 调用成本增加

2. **缺乏数据量控制**：没有对项目数量和数据量进行限制

3. **错误处理不足**：当遇到 token 超限等错误时，缺乏有效的降级处理

## 优化方案

### 1. 数据分批处理

**实现位置**：`server/src/services/aiService.ts`

- 自动检测数据量，如果超过阈值，将数据分成多个批次处理
- 每批最多处理 10 个项目（可通过环境变量配置）
- 每批的 prompt token 数不超过 3000（保守估计，可通过环境变量配置）

**关键方法**：
- `splitDataIntoBatches()`: 将数据分成多个批次
- `generateSummaryInBatches()`: 分批调用 AI API 并合并结果

### 2. 项目数量限制

**实现位置**：`server/src/services/summaryService.ts`

- 默认最多处理 50 个项目（可通过环境变量 `AI_MAX_PROJECTS` 配置）
- 如果项目数量超过限制，只处理前 N 个项目，并记录警告日志

### 3. Prompt 优化

**实现位置**：`server/src/services/aiService.ts` - `buildPrompt()`

- 简化 prompt 格式，减少不必要的文字
- 将事件列表从 Top 5 减少到 Top 3
- 使用更紧凑的数据格式（如 `PV=100, UV=50` 而不是多行描述）

### 4. Token 估算和监控

**实现位置**：`server/src/services/aiService.ts`

- 添加 `estimateTokens()` 方法，粗略估算 prompt 的 token 数量
- 在处理前检查 token 数量，决定是否需要分批处理
- 记录详细的处理日志，包括数据量、token 估算、处理时间等

### 5. 增强的错误处理

**实现位置**：`server/src/services/aiService.ts`

- 检测 token 超限错误，自动切换到分批处理模式
- 增加默认超时时间从 60 秒到 120 秒
- 如果某批处理失败，使用基础总结作为后备方案
- 批次之间添加延迟，避免 API 限流

### 6. 智能总结合并

**实现位置**：`server/src/services/aiService.ts` - `generateSummaryInBatches()`

- 如果多个批次的总结合并后仍然很长，会再次调用 AI 生成一个简洁的总览
- 确保最终输出的总结既完整又简洁

## 环境变量配置

可以在 `server/.env` 文件中配置以下参数：

```env
# AI 总结功能配置
AI_MAX_PROJECTS=50                    # 最多处理的项目数（默认：50）
AI_MAX_PROJECTS_PER_BATCH=10          # 每批最多处理的项目数（默认：10）
AI_MAX_PROMPT_TOKENS=3000             # 最大 prompt token 数（默认：3000）
OPENAI_TIMEOUT_MS=120000              # API 超时时间（毫秒，默认：120000，即 120 秒）
```

## 工作流程

### 正常情况（数据量较小）

1. 收集所有项目的数据
2. 检查数据量和 token 估算
3. 如果数据量在限制内，一次性调用 AI API 生成总结
4. 返回总结结果

### 大数据量情况

1. 收集所有项目的数据（最多 50 个）
2. 检查数据量和 token 估算
3. 如果超过限制，将数据分成多个批次
4. 依次处理每个批次：
   - 调用 AI API 生成该批次的总结
   - 如果失败，使用基础总结作为后备
   - 批次之间延迟 500ms，避免限流
5. 合并所有批次的总结
6. 如果合并后的总结太长，再次调用 AI 生成总览
7. 返回最终总结

## 性能优化效果

### 优化前
- ❌ 数据量大时可能失败（token 超限）
- ❌ 处理时间不可控
- ❌ 没有数据量限制

### 优化后
- ✅ 自动处理大数据量场景
- ✅ 分批处理，避免超时和 token 超限
- ✅ 有数据量限制和监控
- ✅ 更好的错误处理和降级方案
- ✅ 详细的日志记录，便于排查问题

## 使用建议

1. **项目数量控制**：如果您的项目数量很多（>50），建议在设置中指定要总结的项目，而不是总结所有项目

2. **监控日志**：关注服务器日志中的以下信息：
   - `数据量较大，将分批处理`
   - `分为 X 批处理`
   - `总结生成完成，长度: X 字符，耗时: Xms`

3. **环境变量调优**：根据实际使用情况调整环境变量：
   - 如果经常遇到超时，增加 `OPENAI_TIMEOUT_MS`
   - 如果项目数据量很大，减少 `AI_MAX_PROJECTS_PER_BATCH`
   - 如果使用的模型 token 限制更大，可以增加 `AI_MAX_PROMPT_TOKENS`

## 技术细节

### Token 估算方法

使用简单的字符数估算：
- 估算公式：`token 数 ≈ 字符数 / 3`
- 这是一个保守的估算，实际 token 数可能更少
- 对于中文，1 token 约等于 1.5-2 个字符
- 对于英文，1 token 约等于 4 个字符

### 分批策略

1. 优先按项目数量分批（每批最多 10 个项目）
2. 如果单个项目的数据量很大，也会根据 token 估算进行分批
3. 确保每个项目都能被处理（即使单个项目超过限制）

### 错误恢复

- 如果 AI API 调用失败，自动降级到基础总结模板
- 如果某批处理失败，不影响其他批次
- 最终至少会返回基础总结，不会完全失败

## 总结

通过这些优化，AI 总结功能现在可以：
- ✅ 处理任意数量的项目（有上限但可配置）
- ✅ 自动适应数据量大小
- ✅ 在遇到问题时优雅降级
- ✅ 提供详细的处理日志

这些改进确保了功能在大数据量场景下的稳定性和可靠性。







