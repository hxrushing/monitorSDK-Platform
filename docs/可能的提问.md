# 可插拔SDK项目 - 可能的面试提问

根据你简历中提到的项目亮点，面试官可能会从多个角度提问，以下是一些可能的问题，帮助你更好地准备：

---

## 1. **可插拔SDK架构设计**

### 架构设计
- **可插拔设计理念**：你能具体讲讲为什么选择可插拔的架构设计吗？这种设计相比传统的单体SDK有什么优势？在实际项目中带来了哪些好处？
- **探针接口标准化**：你提到所有探针都实现了统一的 `Probe` 接口，能详细解释一下这个接口的设计思路吗？如何保证不同探针之间的互不干扰？
- **动态加载机制**：你是如何实现探针的动态加载的？使用 `import()` 动态导入时，如何确保代码分割和按需加载的效果？构建工具是如何配置的？

### 模块化与解耦
- **探针独立性**：如何确保各个探针模块（错误、HTTP、性能、行为）之间的独立性？如果某个探针初始化失败，会不会影响其他探针的运行？
- **配置系统设计**：配置系统是如何设计的？如何实现配置的合并、验证和默认值处理？如何保证配置的类型安全？

---

## 2. **探针模块实现**

### 错误探针 (ErrorProbe)
- **错误捕获机制**：错误探针是如何捕获 JS Error、Promise Rejection 和资源加载错误的？这些监听器是如何注册和清理的？
- **错误信息采集**：错误探针采集了哪些信息？如何获取源码位置信息？SourceMap 反查是如何实现的？
- **console 重写**：你提到实现了 console 重写但默认关闭，为什么这样设计？重写 console 时需要注意哪些问题？

### HTTP探针 (HttpProbe)
- **请求拦截实现**：如何同时拦截 Fetch API 和 XMLHttpRequest？这两种拦截方式有什么不同？XHR 的请求头为什么无法获取？
- **数据脱敏机制**：HTTP探针如何实现请求头和请求体的脱敏？脱敏规则是如何配置和应用的？如何避免敏感信息泄露？
- **性能指标采集**：如何准确统计请求时长、请求/响应大小？这些指标对性能分析有什么帮助？

### 性能探针 (PerformanceProbe)
- **Web Vitals 集成**：如何集成 Web Vitals 指标（FCP/LCP/CLS/TTFB/INP）？为什么选择动态导入 `web-vitals` 库？
- **长任务监控**：长任务监控是如何实现的？为什么需要采样率控制？30% 的采样率是如何确定的？
- **性能指标上报**：性能指标的上报频率是怎样的？如何避免性能监控本身影响页面性能？

### 行为探针 (BehaviorProbe)
- **路由监听**：如何同时支持 History API、PopState 和 HashChange 的路由监听？不同路由框架（React Router、Vue Router）是如何适配的？
- **页面停留时长**：页面停留时长是如何统计的？如何处理页面可见性变化（visibilitychange）和会话超时？
- **点击事件采集**：为什么点击事件采集默认关闭？如果启用，如何避免产生过多事件影响性能？

---

## 3. **自适应批量上报策略**

### 网络状态检测
- **网络质量评估**：如何检测和评估网络质量？RTT 和带宽是如何测量的？网络质量评级的标准是什么？
- **动态调整算法**：自适应批量大小的调整算法是如何工作的？网络质量、队列长度、发送成功率这三个因素是如何加权合并的？
- **平滑调整机制**：为什么需要平滑调整？如何避免批量大小的剧烈变化？平滑调整的具体实现是怎样的？

### 批量策略优化
- **批量大小范围**：最小批量大小（10）和最大批量大小（100）是如何确定的？在不同网络环境下，批量大小的调整范围是多少？
- **队列管理**：当队列积压时，如何快速清空队列？队列长度对批量大小的影响有多大？
- **发送成功率反馈**：如何根据发送成功率调整批量大小？成功率低时为什么要减小批量？这个策略的实际效果如何？

---

## 4. **指数退避重试机制**

### 重试算法设计
- **指数退避公式**：指数退避的延迟计算公式是什么？为什么选择 2 作为乘数？最大延迟（30秒）是如何确定的？
- **错误类型分类**：如何区分网络错误、超时错误、服务器错误和客户端错误？不同错误类型的重试策略有什么不同？
- **网络感知调整**：如何根据网络状况调整重试延迟？网络好时和网络差时的延迟调整倍数是多少？

### 抖动机制
- **雷群效应**：什么是雷群效应？为什么需要抖动机制来避免雷群效应？抖动范围是如何计算的？
- **抖动实现**：抖动的具体实现是怎样的？如何确保抖动后的延迟不会太小或太大？

### 重试效果
- **实际效果**：指数退避重试机制在实际项目中提升了多少成功率？相比固定延迟重试，有什么优势？
- **资源消耗**：重试机制如何避免无效的资源消耗？如何平衡重试次数和最终成功率？

---

## 5. **性能与可靠性的平衡**

### 核心权衡策略
- **批量上报 vs 数据时效**：如何平衡批量上报的网络开销和数据时效性？5秒定时刷新和50条批量阈值是如何确定的？
- **离线存储策略**：为什么正常运行期间数据只在内存队列？什么情况下才会写入 localStorage？1MB 的容量限制是如何确定的？
- **数据压缩机制**：数据压缩的阈值（100字节）是如何确定的？为什么小数据不压缩？压缩算法是如何选择的？

### 前端性能保护
- **异步非阻塞**：如何确保 SDK 运行对主业务"零感"？Fetch 异步化和 Beacon 兜底是如何实现的？
- **优先级机制**：事件优先级（high/normal/low）是如何实现的？当网络极差时，如何确保关键数据优先上报？
- **异常隔离**：如何避免 SDK 自身的错误导致宿主页面崩溃？如何防止递归上报的死循环？

### 典型场景处理
- **页面关闭场景**：页面关闭时如何确保数据不丢失？Beacon API 的使用有什么注意事项？
- **极端弱网场景**：极端弱网下如何平衡可靠性和性能？离线存储的 FIFO 策略是如何实现的？
- **存储溢出场景**：当 localStorage 即将溢出时，如何处理？如何保护用户设备不卡死？

---

## 6. **传输管道与事件队列**

### 事件队列管理
- **队列数据结构**：事件队列是如何实现的？如何保证不同优先级事件的正确排序？
- **队列容量控制**：队列的最大容量是多少？如何防止队列无限增长？队列满时的处理策略是什么？

### 传输管道设计
- **批量上报实现**：批量上报是如何实现的？如何将多个事件合并成一个请求？批量请求的格式是什么？
- **Beacon 兜底机制**：什么情况下使用 Beacon API？Beacon 和 Fetch 的区别是什么？如何确保 Beacon 的可靠性？
- **离线缓存机制**：离线缓存是如何实现的？如何压缩和存储离线数据？网络恢复后如何恢复上报？

---

## 7. **数据压缩与优化**

### 压缩策略
- **压缩阈值**：为什么设置 100 字节的压缩阈值？小数据不压缩的原因是什么？
- **压缩算法选择**：优先使用 `CompressionStream` 的原因是什么？不支持时如何回退到自定义压缩？压缩效果如何？
- **CPU 负载平衡**：如何平衡压缩带来的带宽节省和 CPU 负载？压缩对性能的影响有多大？

### 数据优化
- **Payload 大小限制**：HTTP 探针的 `maxBodySize`（10KB）是如何确定的？如何避免过大的请求体影响性能？
- **采样率控制**：不同探针的采样率是如何配置的？采样率对数据准确性的影响如何评估？

---

## 8. **框架适配与兼容性**

### 路由适配
- **多框架支持**：如何同时支持 React Router、Vue Router 和原生路由？适配层的设计思路是什么？
- **路由监听兼容**：如何兼容 History API、PopState 和 HashChange？不同路由模式下的监听策略有什么不同？

### 浏览器兼容
- **API 兼容性**：如何检测浏览器是否支持 PerformanceObserver、Beacon API 等新特性？不支持时如何降级？
- **Polyfill 策略**：是否需要引入 Polyfill？如何控制 SDK 的体积？

---

## 9. **SDK 构建与发布**

### 构建配置
- **多格式输出**：如何同时输出 UMD、ESM 和 IIFE 格式？不同格式的使用场景是什么？
- **代码分割**：如何确保探针模块可以独立打包？构建工具的配置是怎样的？
- **类型声明**：如何生成 TypeScript 类型声明文件？类型定义的完整性如何保证？

### 发布与分发
- **npm 发布**：SDK 如何发布到 npm？版本号管理策略是什么？
- **CDN 分发**：CDN 分发的实现方式是什么？如何确保 CDN 版本的稳定性和可用性？

---

## 10. **测试与监控**

### 测试策略
- **单元测试**：各个探针模块如何测试？如何模拟不同的错误场景和网络环境？
- **集成测试**：SDK 的集成测试是如何进行的？如何测试不同框架下的适配效果？
- **性能测试**：如何测试 SDK 对页面性能的影响？性能开销的评估标准是什么？

### 监控与调试
- **调试能力**：SDK 提供了哪些调试能力？如何查看当前批量大小、网络状态、重试统计等信息？
- **手动 Flush**：手动 Flush 功能是如何实现的？在什么场景下需要使用手动 Flush？

---

## 其他可能的追问

### 技术选型
- **技术栈选择**：为什么选择 TypeScript？为什么使用 Fetch 而不是 XMLHttpRequest？构建工具的选择（Vite/Webpack）有什么考虑？
- **依赖管理**：SDK 的依赖是如何管理的？如何控制 SDK 的体积？如何避免依赖冲突？

### 实际应用
- **项目落地**：这个 SDK 在实际项目中是如何落地的？遇到了哪些问题？如何解决的？
- **性能指标**：SDK 在实际使用中的性能指标如何？对页面加载时间、内存占用等的影响有多大？
- **数据准确性**：如何保证采集的数据的准确性？采样率对数据分析的影响如何评估？

### 扩展与优化
- **未来规划**：SDK 的未来规划是什么？还有哪些功能需要完善？录屏/回放、Session 关联等功能如何实现？
- **性能优化**：还有哪些性能优化的空间？如何进一步减小 SDK 的体积？
- **可靠性提升**：如何进一步提升数据的可靠性？离线场景下的数据恢复策略是什么？

---

## 总结

通过准备这些问题，你不仅能够更好地展示你的项目经验，还能展示你对每个技术亮点的深入理解和实际应用能力。建议：

1. **准备具体案例**：每个技术点都准备一个具体的实现案例或遇到的问题
2. **量化效果**：准备一些量化的数据，如性能提升百分比、成功率提升等
3. **深入思考**：思考每个设计决策背后的原因，以及可能的改进方向
4. **实践验证**：确保你能够清晰地解释每个功能的实现原理和实际效果

如果需要进一步细化某个问题的回答，随时告诉我！

